\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithm,algorithmic}% http://ctan.org/pkg/algorithms
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}

\DeclareMathOperator*{\argmin}{argmin}
\DeclareMathOperator*{\minimize}{min}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\begin{document}

\title{Reinforcement Learning in Adaptive Control}

\author{\IEEEauthorblockN{Chang-Shao Shen}
\IEEEauthorblockA{\textit{Dept. of Electrical Engineering} \\
\textit{National Taiwan University}\\
Taipei, Taiwan \\
Email: b04901088@ntu.edu.tw}
\and
\IEEEauthorblockN{Bing-Jyue Chen}
\IEEEauthorblockA{\textit{Dept. of Electrical Engineering} \\
	\textit{National Taiwan University}\\
	Taipei, Taiwan \\
	Email: b04901087@ntu.edu.tw}
}
\maketitle

\begin{abstract}
This paper reviews the current progress on reinforcement learning (RL) based feedback control solutions to regulation and tracking problems to design controllers for discrete-time (DT) linear/nonlinear dynamical systems that combine features of adaptive control and optimal control. Discussion will mainly focus on state feedback methods. Output feedback, however, will also be reviewed. Moreover, lyapunov-based RL control for stability will be covered as well. Core algorithms will be based on Q-learning and actor-critic method.
\end{abstract}

\begin{IEEEkeywords}
reinforcement learning (RL), optimal adaptive control
\end{IEEEkeywords}

\section{Introduction}
Reinforcement learning is a framework in machine learning to tactically modify the actions of an agent based on the observed response from the environment it is interacting with. RL methods stem from the research from psychology of human decision making. One interacts with the environment and learn optimal behaviors to maximize the positive effect of the response.
\par In control engineering paradigm, RL refers to the process of learning optimal controller based on the response from the environment without knowing the system dynamics, which is also called \textit{action-based learning}. Previous work \cite{b1} has shown that reinforcement learning control is direct adaptive optimal control. Optimal control is generally an offline deisgn technique that requires full knowledge of system dynmaics to solve HJB equations. On the other hand, adaptive controllers learn online to control unknown system using data measured along the system trajectories, while not gauranteeing the input to be optimal in the sense of minimizing prescribed cost function. With reinforcement learning, a controller is able to solve HJB equations online without knowledge of system dynamics and adapt the system input to that derived from optimal control theory.
\par This paper is organized as follows. Section \ref{background} briefly reviews some background theory in reinforcement learning, i.e., Markov decision process, Bellman equation, and value iteration. Section \ref{sol} presents solutions to discrete-time (DT) optimal control problems, including regulation and tracking problem for both linear and nonlinear systems using RL algorithms. Finally in Section \ref{improve}, miscellaneous improvement of RL methods including lyapunov-constrained algorithm for stability and output feedback algorithm for partially observable states will be introduced, followed by concluding remarks in Section \ref{conclusion}. 
\section{background theory} \label{background}

\subsection{Reinforcement Learning}

The study of reinforcement learning can be based on Markov decision process (MDP), which can be started by defining optimal decision problems, where decisions are made at stages of a process evolving through time, and total cost is accumulated along the trajectory. Consider the MDP \( (X, U, P, R) \), where $X$ is a set of state, and $U$ is a set of actions or control input. $P:X\times U\times X \rightarrow [0, 1]$ is the transition probability from state $x$ to state $x'$ under input $u$, also denoted as \(Pr\{x'|x, u\}\). Here we assumed the transition is always deterministic. That is, \(Pr\{x'|x, u\}\) is always 1. $R:X\times U\times X \rightarrow \mathbb{R}$ is the cost-to-go picked up after state transition from $x$ to  $x'$ under input $u$. 
The mapping from state to action is called \textit{policy}, denoted as $\pi$. A policy gives the conditional probability \(\pi(x, u) = Pr\{u|x\} \) of taking action $u$ in state $x$. Here we also assume the policy is always deterministic. The goal of reinforcement learning is to minimize accumulated cost-to-go along the trajectory by finding optimal policy \(\pi^*\).
\par To find the optimal policy, we define the following functions.
\begin{equation}
\begin{aligned}
	& J_k = \sum_{i=0}^\infty\gamma^{i-k}r_i\\
	& V^\pi(x_k) = E_\pi\{J_k|x = x_k\} \label{value}
\end{aligned}
\end{equation}
where $J_k$ is the total infinite-horizon cost starting from time $k$, and \(\gamma \in \mathbb{R},\; 0 < \gamma \leq 1\) is the discount factor, and $r_i$ is the cost-to-go for every state transition. $V^\pi(x_k)$, called \textit{value function}, is the expected value of $J_k$ under policy $\pi$. Qualitatively, value function is an estimation of the long term cost a state expects. We can find the optimal policy by minimizing the value function
\[\pi^* = \argmin_{\pi}V^\pi(x_k) \]
And the corresponding optimal value function is
\[V^*(x_k) = \min_{\pi}V^\pi(x_k)\]
\par After some derivation, the definition of value function in \eqref{value} can be expressed as 
\begin{equation}
  V^\pi(x_k) = r_k + V^\pi(x_{k+1}) \label{bellman}
\end{equation}
under the assumption of deterministic policy and transition probability. \eqref{bellman} is called \textit{Bellman equation}, which is the fundamental of RL algorithms. It provides a backward recursion for the value at time $k$ in terms of the value at time $k+1$.
\par With principle of optimality, the optimal value function must satisfy
\begin{equation}
	V^{*}(x_k) = \minimize_{u_k}[r_k+V^{*}(x_{k+1})]
\end{equation}
which is a fixed-point equation. Therefore it can be solved by successive approximation using an algorithm called \textit{value iteration}, which will be introduced in the next section.
\section{Solutions to DT Optimal control using RL} \label{sol}
Consider the following linear and nonlinear systems:
\begin{equation}
x_{k+1} = Ax_{k} + Bu_{k}\label{linear}
\end{equation}
\begin{equation}
x_{k+1} = f(x_{k}) + g(x_{k})u_{k}\label{nonlinear}
\end{equation}
where \(x_{k} \in \mathbb{R}^{n}\) is the state vector, \(u_{k} \in \mathbb{R}^{m}\) is the control input, \(A \in \mathbb{R}^{n\times n}\), \(f(x_{k}) \in \mathbb{R}^{n}\) are the drift dynmaics, \(B \in \mathbb{R}^{n\times m}\), \(g(x_{k}) \in \mathbb{R}^{n\times m} \) are the input dynamics.
It is assumed that the system is stabilizable and \((A, B)\) are controllable. Output feedback will be discussed in Section \ref{improve} and thus output dynamics(often denoted as \(C\) matrix) is not introduced here.

\subsection{Optimal Regulation Problem}
The goal of optimal regulation problem is to design an input that stabilizes the state to a desired target(often zero), while minimizing the predesigned performance function, i.e. long term cost function, which can be defined as
\begin{equation}
	J = \sum_{i=0}^{\infty}r(x_i, u_i) = \sum_{i=0}^{\infty}(Q(x_i) +u_{i}^{T}Ru_{i})
\end{equation}
where \(Q(x_i)\succeq 0\), \(R=R^T \succ 0\) are predefined state cost-to-go function and input cost matrix respectively. The value function can be defined as
\begin{equation}
	V(x_k) = \sum_{i=k}^{\infty}r(x_i, u_i) = \sum_{i=k}^{\infty}(Q(x_i) +u_{i}^{T}Ru_{i}), \forall x
\end{equation}
which can also be formulated as Bellman equation
\begin{equation}
	V(x_k) = r(x_k, u_k) + V(x_{k+1})\label{cost}
\end{equation}
Thus, with principle of optimality, the problem reduces to finding optimal value function.
\begin{equation}
	V^{*}(x_k) = \minimize_{u_k}[r(x_k, u_k)+V^{*}(x_{k+1})]\label{dthjb}
\end{equation}
which is the DT HJB equation.
For DT linear system as in \eqref{linear}, typical input cost function is defined as \(Q(x_k) = x^{T}_kQx_k, Q \succ 0\). Then the value function is shown to be quadratic.
\begin{equation}
	V(x_k) = x^{T}_kPx_k\label{lqvalue}
\end{equation}
where \( \quad P=P^T \succ 0\) is called kernel matrix. Putting \eqref{lqvalue} into \eqref{dthjb} and solve the equation yields the DT Algebraic Riccati equation(DARE)
\begin{equation}
Q - P + A^{T}PA - A^TPB(R+B^TPB)^{-1}B^TPA = 0\label{dare}
\end{equation}
and the optimal control input is 
\begin{equation}
\begin{split}
	u^{*}_k = \argmin_{u_k}[r((x_k, u_k)+V^{*}(x_{k+1}))]\\
	= -(R+B^TPB)^{-1}B^TPAx_k
\end{split}
\end{equation}
In the case of nonlinear system in \eqref{nonlinear}. The optimal control input is
\begin{equation}
\begin{split}
	u^{*}_k = \argmin_{u_k}[r((x_k, u_k)+V^{*}(x_{k+1}))]\\
	=  -\frac{1}{2}R^{-1}g^{T}(x)(\frac{\partial{V(x_{k+1})}}{\partial{x_{k+1}}})
\end{split}
\end{equation}
The following presents methods for solving HJB equation online using RL.

\subsubsection{Value Iteration (VI)}
Algorithm \ref{alg:dtvi} provides a solution to solve HJB equation online using value iteration but requires full knowledge of system dynamics.
\begin{algorithm}
	\caption{VI Algorithm to solve HJB}
	\label{alg:dtvi}
	\begin{algorithmic}[1]
		\STATE Select a initial control policy \(u^0_k\).
		\STATE For \(j=0,1, ...\), perfrom until convergence: \\
		 \textbf{Value Update:}
		 \[
		  V^{j+1}(x_k) = Q(x_k) + (u^j_k)^TRu^j_k + V^{j}(x_{k+1})\]
		  on convergence, set \(V^{j+1}(x_k)=V^j(x_k)\).
		\STATE \textbf{Policy Improvement:}
		\[u^{j+1}_k = -(R+B^TPB)^{-1}B^TPAx_k\quad\text{(linear case)}\] 
		\[u^{j+1}_k = -\frac{1}{2}R^{-1}g^{T}(x)(\frac{\partial{V(x_{k+1})}}{\partial{x_{k+1}}})\quad\text{(nonlinear case)}\] 
		\STATE Go to 2
	\end{algorithmic}
\end{algorithm}
\subsubsection{Q Learning for the DT LQR} \label{qlearing}
Q Learning is model-free learning approach which converges to the global optimal solution under the condition of persistence exciting (PE) signals. Q function is an action-dependent value function, representing how positive a state-action pair is. DT Q function is defined as 
	\[Q(x_k, u_k) = r(x_k, u_k) + V(x_{k+1})\]
Let \(z_k = [x^T_k z^T_k]^T\), generally the Q function can be paramterized as
	\[Q(x, u) = W^T\phi(z) = \sum_{i=1}^{N}w_i\varphi_i(z)\]
where \(\phi(z) = [\varphi_1(z) \; \varphi_2(z) \dots \varphi_N(z)]\) with \(\varphi_i(z)\) as basis functions. \(N\) is the number of neurons, and \(w_i\) are the neural network weights. This is called \textit{value function approximation (VFA)} technique, often used for continuous state action space in which selecting action from discrete action space and updating value function from discrete state space is computationally expensive or impossible.
\par For the problem of linear quadratic regulation (LQR), from \eqref{cost} and \eqref{lqvalue}, the Q function is 
	\[Q(x_k, u_k) = x^T_kQx_k + u^T_kRu_k + x^T_{k+1}Px_{k+1}\]
Apply the dynamics in \eqref{linear}, the Q function becomes
\[Q(x_k, u_k) = \begin{bmatrix}
x_k\\
u_k
\end{bmatrix}^T
\begin{bmatrix}
	Q+A^TPA & A^TPB\\
	B^TPA & R+B^TPB\\
\end{bmatrix}
\begin{bmatrix}
	x_k\\
	u_k
\end{bmatrix}\]
Define a kernel matrix \(S\) as
\[Q(x_k, u_k) = \begin{bmatrix}
x_k\\
u_k
\end{bmatrix}^T
\begin{bmatrix}
S_{xx} & S_{xu}\\
S_{ux} & S_{uu}\\
\end{bmatrix}
\begin{bmatrix}
x_k\\
u_k
\end{bmatrix} \\
= z^T_kSz_k\]
Thus, for DT LQR, the Q function is better parameterized as

\[Q(z_k) = z^T_kSz_k\]

Solving \(\frac{\partial{Q}}{\partial{u_k}} = 0\) yields the optimal control input
\[u^{*}_k = -S^{-1}_{uu}S_{ux}x_k\]
\begin{algorithm}
	\caption{Q Learning algortihm to solve DARE}
	\label{alg:qdtlqr}
	\begin{algorithmic}[1]
		\STATE Select a initial control policy \(u^0_k\).
		\STATE For \(j=0,1, ...\), perfrom until convergence: \\
		\textbf{Value Update:}
		\[
		z^T_kS^{j+1}z_k = x^T_kQx_k + u^T_kRu_k + z^T_{k+1}S^{j}z_{k+1}\]
		on convergence, set \(S^{j+1}=S^j\).
		\STATE \textbf{Policy Improvement:}
		\[u^{*}_k = -S^{-1}_{uu}S_{ux}x_k\]
		\STATE Go to 2
	\end{algorithmic}
\end{algorithm}
The value step can be implemented with batch LS, recursive LS, and gradient methods using data collected along the state trajecories.
\subsubsection{Actor-Critic Approximators}
We can extend the idea of VFA described in \ref{qlearing} to \textit{policy approximation} to further generalize the system and avoid the requirement of knowledge of plant dynamics. This method is called \textit{Actor-Critic} method, in which two approximators --- value function approximator and policy approximator are structured. The value function approximator is maintained by \textit{critic} and estimates the value function by being updated to minimizing the temporal difference error. The policy approximator is maintained by \textit{actor} and estimates the control input from the current state based on the current value function while minimizing the value function. The two approximators can be represented as follows.
\begin{equation}
\begin{aligned} \label{ac}
	V^{j}(x_k) = (W^j_c)^T\phi_c(x_k) = \sum_{i=1}^{N_c}w^j_c\varphi_{ci}(x_k), \quad \forall x \\
	u^j_k = (W^j_a)^T\sigma_a(x_k) = \sum_{i=1}^{N_a}w^j_a\varsigma_{ai}(x_k), \quad \forall x 
\end{aligned}
\end{equation}
where \(\varphi_{ci}(x_k)\) and \(\varsigma_{ai}(x_k)\) are the basis functions,
\(W^j_c= 
\begin{bmatrix}
w^j_{c1} w^j_{c2} \dots w^j_{cNc}
\end{bmatrix}\), 
\(W^j_a= 
\begin{bmatrix}
w^j_{a1} w^j_{a2} \dots w^j_{aNa}
\end{bmatrix}\) are the weight vectors of the approximators with \(N_c\) being the number of neurons of critic and \(N_a\) being the number of neurons of actor.
Thus, the Bellman equation of value function approximator can be written as
\[(W^{j+1}_c)^T\phi_c(x_k) = r(x_k, u^j_k) + (W^{j+1}_c)^T\phi_c(x_{k+1})\]
And the gradient adaptive laws for actor and critic are derived as
\begin{equation}
\begin{aligned}
	W^{j+1}_c \gets W^{j+1}_c - \alpha_1\phi_c(x_k)(( W^{j+1}_c)^T\phi_c(x_k) \\- r(x_k, u^j_k))
\end{aligned}
\end{equation}
\begin{equation}
\begin{aligned}
	W^{j+1}_a \gets W^{j+1}_a - \alpha_2\sigma_a(x_k)\Big(2R( W^{j+1}_a)^T\sigma_a(x_k) + \\ g^T(x_k)\frac{\partial{\phi_c(x_{k+1})}}{\partial{x_{k+1}}}W^{j+1}_c\Big)^T
\end{aligned}
\end{equation}
where \(\alpha_1, \alpha_2 > 0\) are the tuning gains, also known as learning rates. It can be observed that with the actor-critic structure, state drift dynamics \(f(\cdot)\) is not required to obtain an optimal control input.
\par The structure of the value function approximator plays an important role in convergence and performance of value iteration. If the structure is designed inappropriately, it may never converge to an optimal solution. For linear systems, the form of value function is shown to be quadratic as in \eqref{lqvalue}. For nonlinear systems, the approximation can be single-layer neural network with a large number of neurons or a multi-layer neural to gaurantee a acceptable approximation error.
\subsection{Optimal Tracking}
The goal of optimal tracking is to make the state \(x_k\) follow the desired tracjectory \(x^d_k\). Define the error \(e_k\) as 
\[e_k = x_k - x^d_k\]
Optimal control input is designed to minmize the cost functional
\[J = \sum_{i=0}^\infty(e^T_k(i)Qe_k(i) + u^T_k(i)Ru_k(i))\]
Traditional solutions involve a feedback term and a feedward term, which requires complete knowledge of the system dynamics and the reference dynamics. In \cite{b8}, a new solution is developed for linear tracking using Q learning without full knowledge of system dynamics. The solution is described as follows. Consider the following augmented states
\[X_k = \begin{bmatrix}
x_k \\
x^d_k
\end{bmatrix}\]
\[Z_k =\begin{bmatrix}
X_k\\
u_k
\end{bmatrix} \]
Define Q function as
\[Q(X_k, u_k) = \frac{1}{2}X^T_kQ_1X_k + \frac{1}{2}u^T_kRu_k +\gamma Q(X_{k+1}, u_{k+1})\]
where \(\gamma \in \mathbb{R},\; 0 < \gamma \leq 1\) is the discount factor, \(Q_1 = [I -I]^TQ[I -I]\).
Q function can also be written as 
\[Q(Z_k) = \frac{1}{2}Z^T_kHZ_k \]
And Bellman equation can be derived.
\[Z^T_kHZ_k = X^T_kQ_1X_k + u^T_kRu_k + \gamma Z^T_{k+1}HZ_{k+1} \]
Then apply algorithm \ref{alg:qdtlqr} with respect to the above modified Q function leads to optimal tracking solution. Note that an actor-critic structure similar to \eqref{ac} can also be implemented to obtain optimal control input online.

\section{Miscellaneous improvements on RL control} \label{improve}
\subsection{Lyapunov-constrained Algorithm for Stability}
In RL there is a tradeoff between exploitation and exploration of policy. Exploitation ensures the optimization of the policy, while exploration chooses random action with the aim of finding optimal state that may not be reached with greedy optimization of policy. Therefore, the agent can choose an action that lead to instability of the system. To cope with this problem, we can impose lyapunov stability condition to constraint action space into a stability-assured action space. Lyapunov approach starts with the construction of a scalar function known as lyapunov candidate function, which is always positive definite with negative time derivative. That is, a lyapunov candidate function \(L(x)\) musy satisfy the following conditions
\begin{equation}
\begin{aligned}
	& L(0) = 0 \quad \\
	& L(x) > 0, \; x \neq 0 \\
	& \dot{L}(x) < 0, \; x \notin 0
\end{aligned}
\end{equation}
In DT system, \(\dot{L}(x)\) is interpreted as the  difference
 \[L(x_{k+1}) - L(x_k)\]
In \cite{b7}, cost-to-go function \(r(x, u) = r(x)\) is selected as lyapunov candidate function \(L(x)\) to constraint the action space for stability.
\subsection{Output feedback Algorithm for Partially Observable States}
In practice, full state information is usually not available. Thus, methods using only input/output data is needed for realistic use. In control theory, these types of methods are called output feedback, also termed as partially observable Markov decision processes in RL.
\par In \cite{b4}, an linear quadratic controller is developed in the form of autoregressive–moving-average (ARMA) controller, in which input/output sequences are applied to realize output feedback.
\[
\overline{u}_{k-1, k-N} = 
\begin{bmatrix} 
u_{k-1}\\
u_{k-2} \\
\vdots \\
u_{k-N}
\end{bmatrix},\;
\overline{y}_{k-1, k-N} = 
\begin{bmatrix} 
y_{k-1}\\
y_{k-2} \\
\vdots \\
y_{k-N}
\end{bmatrix}
\]
\[\overline{z}_{k-1, k-N} = 
\begin{bmatrix} 
\overline{u}_{k-1, k-N}\\
\overline{y}_{k-1, k-N}
\end{bmatrix}
\]
where \(\overline{u}_{k-1, k-N}\) and \(\overline{y}_{k-1, k-N}\) are the input/output sequences over the time interval \([k-N, k-1]\), \(\overline{z}_{k-1, k-N}\) is the augmented state. Then, a augmented kernel matrix for Q function can be developed with \(\overline{z}_{k-1, k-N}\) for Q learning.
\section{Conclusion} \label{conclusion}
In this paper, we have reviewed serveral methods using reinforcement learning technique to bring together optimal control and adaptive control, with solutions to DT regulation and tracking problems as demonstration. HJB equations, which are generally solved offline and require precise model of the system in optimal control, can be solved online using data collected along the trajectory without full knowledge of the system dynamics with RL-based methods. In addition, we also review improvements on these methods, including stability-assured RL controller with lyapunov criterion and output feedback controller with ARMA.



\begin{thebibliography}{00}
\bibitem{b1} Sutton RS, Barto AG, Williams RJ (1992) Reinforcement learning is
direct adaptive optimal control. IEEE Control Systems 12(2): 19-22.
\bibitem{b2} B. Kiumarsi, K. G. Vamvoudakis, H. Modares and F. L. Lewis, "Optimal and Autonomous Control Using Reinforcement Learning: A Survey," in IEEE Transactions on Neural Networks and Learning Systems, vol. 29, no. 6, pp. 2042-2062, June 2018.
\bibitem{b3} F. L. Lewis, D. Vrabie and K. G. Vamvoudakis, "Reinforcement Learning and Feedback Control: Using Natural Decision Methods to Design Optimal Adaptive Controllers," in IEEE Control Systems, vol. 32, no. 6, pp. 76-105, Dec. 2012.
\bibitem{b4} F. L. Lewis and K. G. Vamvoudakis, "Reinforcement Learning for Partially Observable Dynamic Processes: Adaptive Dynamic Programming Using Measured Output Data," in IEEE Transactions on Systems, Man, and Cybernetics, Part B (Cybernetics), vol. 41, no. 1, pp. 14-25, Feb. 2011.
\bibitem{b5} J. Hall, C. E. Rasmussen and J. Maciejowski, "Reinforcement learning with reference tracking control in continuous state spaces," 2011 50th IEEE Conference on Decision and Control and European Control Conference, Orlando, FL, 2011, pp. 6019-6024.
\bibitem{b6} Y. P. Pane, S. P. Nageshrao and R. Babuška, "Actor-critic reinforcement learning for tracking control in robotics," 2016 IEEE 55th Conference on Decision and Control (CDC), Las Vegas, NV, 2016, pp. 5819-5826.
\bibitem{b7} A. Kumar and R. Sharma, "A stable Lyapunov constrained reinforcement learning based neural controller for non linear systems," International Conference on Computing, Communication \& Automation, Noida, 2015, pp. 185-189.
\bibitem{b8} Bahare Kiumarsi, Frank L. Lewis, Hamidreza Modares, Ali Karimpour, Mohammad-Bagher Naghibi-Sistani,
Reinforcement Q-learning for optimal tracking control of linear discrete-time systems with unknown dynamics,
Automatica,
Volume 50, Issue 4,
2014,
Pages 1167-1175.

\end{thebibliography}

\end{document}
